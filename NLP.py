# -*- coding: utf-8 -*-
"""Untitled19.ipynb

Automatically generated by Colab.

Original file is located at
https://colab.research.google.com/drive/1u5sf7joCW4y_8zNJRvtE4huEjeNLxcbh?usp=sharing
"""

!pip install transformers datasets torch

from google.colab import files
files.upload()

!mkdir -p ~/1.kaggle
!cp kaggle.json ~/1.kaggle/
!chmod 600 ~/1.kaggle/kaggle.json

import os
os.environ['KAGGLE_CONFIG_DIR'] = "/content"
!mkdir -p /content/.kaggle
!cp kaggle.json /content/.kaggle/
!chmod 600 /content/.kaggle/kaggle.json

!kaggle datasets list

!kaggle datasets download -d akshatgupta7/llm-fine-tuning-dataset-of-indian-legal-texts

!unzip llm-fine-tuning-dataset-of-indian-legal-texts.zip

import pandas as pd

# Load the first JSON file into a DataFrame
df1 = pd.read_json('/content/constitution_qa.json')

# Load the second JSON file into a DataFrame
df2 = pd.read_json('/content/crpc_qa.json')

# Load the third JSON file into a DataFrame
df3 = pd.read_json('/content/ipc_qa.json')

# Check the first few rows of each dataframe
print(df1.head())
print(df2.head())
print(df3.head())

def convert_to_ner_format(df):
    """
    Convert dataset into the NER format:
    [{'text': 'sentence', 'entities': [(start, end, 'entity_type')]}]
    """
    ner_data = []

    # For each text entry in the dataset
    for index, row in df.iterrows():
        text = row['question']  # Assuming the text is in the 'question' column
        entity_list = []

        # Access the answer which contains the entities
        answer = row['answer']

        # Assuming answer has a structure containing entities
        if 'entities' in answer:
            # Loop through the entities and extract their start, end, and type
            for entity in answer['entities']:
                start = entity['start']
                end = entity['end']
                entity_type = entity['label']  # Entity type (e.g., PERSON, LOCATION)
                entity_list.append((start, end, entity_type))

        ner_data.append({
            'text': text,
            'entities': entity_list
        })

    return ner_data

# Choose the desired DataFrame (df1, df2, or df3)
df = df1  # For example, using df1

# Now you can call the function with the correct DataFrame
ner_format = convert_to_ner_format(df)

# Print a sample entry
print(ner_format[0])

import torch
from torch.utils.data import Dataset, DataLoader
from transformers import BertTokenizer

# Initialize tokenizer (if using BERT, you can change the model)
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

class NERDataset(Dataset):
    def __init__(self, data, tokenizer, max_length=128):
        self.data = data
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        text = self.data[idx]['text']
        entities = self.data[idx]['entities']

        # Tokenize the text
        encodings = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')

        # Create entity labels (for NER)
        labels = [-100] * self.max_length  # Initialize with -100 (ignore index for padding tokens)

        for start, end, label in entities:
            start_token = encodings.char_to_token(start)
            end_token = encodings.char_to_token(end)
            if start_token is not None and end_token is not None:
                labels[start_token:end_token + 1] = [label] * (end_token - start_token + 1)

        return {
            'input_ids': encodings['input_ids'].flatten(),
            'attention_mask': encodings['attention_mask'].flatten(),
            'labels': torch.tensor(labels)
        }

# Create the dataset and dataloader
dataset = NERDataset(ner_format, tokenizer)
dataloader = DataLoader(dataset, batch_size=16)

# Example: View a sample batch
for batch in dataloader:
    print(batch)
    break

from sklearn.model_selection import train_test_split

# Assuming 'dataset' is a list of tokenized examples
train_data, val_data = train_test_split(dataset, test_size=0.1)

from torch.utils.data import DataLoader

train_loader = DataLoader(train_data, batch_size=8, shuffle=True)
val_loader = DataLoader(val_data, batch_size=8)

from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="./legal_model_output",   # Directory to save the model and logs
    evaluation_strategy="epoch",         # Evaluate at the end of each epoch
    per_device_train_batch_size=8,       # Batch size for training
    per_device_eval_batch_size=8,        # Batch size for evaluation
    num_train_epochs=3,                  # Number of training epochs
    logging_dir='./logs',                # Directory for logs
    logging_steps=10,                    # Log every 10 steps
    save_steps=1000,                     # Save model every 1000 steps
    save_total_limit=2,                  # Limit the number of saved models
)

def tokenize_and_align_labels(examples):
    # Tokenize the text
    tokenized_inputs = tokenizer(examples['text'], truncation=True, padding=True, max_length=128, return_tensors='pt')

    # Check the tokenized inputs
    if len(tokenized_inputs['input_ids'][0]) == 0:
        print(f"Empty input tokens for text: {examples['text']}")

    # Initialize labels (same size as tokenized input)
    labels = [-100] * len(tokenized_inputs.input_ids[0])  # -100 is used to ignore padding tokens

    # Align the entities with the tokenized words
    for entity in examples['entities']:
        start, end, label = entity['start'], entity['end'], entity['label']

        # Find the token indices for the entity
        start_token = tokenized_inputs.char_to_token(start)
        end_token = tokenized_inputs.char_to_token(end)

        # If both start and end tokens exist
        if start_token is not None and end_token is not None:
            labels[start_token:end_token + 1] = [label] * (end_token - start_token + 1)

    # Check if the labels match the length of the tokenized input
    if len(labels) != len(tokenized_inputs['input_ids'][0]):
        print(f"Label mismatch for text: {examples['text']}")

    # Add labels to the tokenized input
    tokenized_inputs["labels"] = labels
    return tokenized_inputs

# Example of tokenization output
text = "John Doe filed a case in the Supreme Court."
encoded_input = tokenizer(text, return_tensors='pt')
print(encoded_input)

import torch
from torch.utils.data import DataLoader
from transformers import AutoTokenizer, AutoModelForTokenClassification

# Load the tokenizer and model
tokenizer = AutoTokenizer.from_pretrained('nlpaueb/legal-bert-base-uncased')
model = AutoModelForTokenClassification.from_pretrained('nlpaueb/legal-bert-base-uncased')


# Dummy dataset structure
temp = [{'text': "John Doe filed a case in the Supreme Court.", 'entities': [{'start': 0, 'end': 8, 'label': 'PERSON'}, {'start': 29, 'end': 44, 'label': 'ORG'}]}]

# Apply the tokenization function
processed_data = [tokenize_and_align_labels(entry) for entry in dataset]
# val_data = [tokenize_and_align_labels(entry) for entry in val_dataset] # Assuming you have a val_dataset defined

# Check the processed data
print(processed_data)

# Create DataLoader for training
train_loader = DataLoader(processed_data, batch_size=8, shuffle=True)

# Check the first batch
for batch in train_loader:
    print(batch)
    break

print(dataset)

""" "crpc": "/content/crpc_qa.json",
        "ipc": "/content/ipc_qa.json",
        "cons": "/content/constitution_qa.json"
"""

from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments

# Step 1: Load the dataset (you should adjust paths or dataset identifiers)
dataset = load_dataset(
    "json",
    data_files={
        "crpc": "/content/crpc_qa.json", "ipc": "/content/ipc_qa.json", "cons": "/content/constitution_qa.json"
          # Replace with your actual file paths
    }
)

# Step 2: Load the pretrained model and tokenizer
model_name = 'nlpaueb/legal-bert-base-uncased'
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=9)  # Adjust num_labels

# Step 3: Define the tokenization and label alignment function
def tokenize_and_align_labels(examples):
    # Tokenize the inputs (text). Padding and truncation is handled here.
    tokenized_inputs = tokenizer(
        examples['question'],  # Adjust key based on dataset column
        padding="max_length",  # Pad all sequences to the same length
        truncation=True,       # Truncate sequences that are too long
        max_length=128,        # You can change max_length based on your dataset
        is_split_into_words=False
    )

    # Create the label array with padding tokens marked as -100
    labels = []
    for idx, text in enumerate(examples['question']):
        # Assuming your label format is aligned with tokenized input
        # If you have other label logic, update it here
        label = [-100] * len(tokenized_inputs['input_ids'][idx])  # Set -100 for padding tokens
        labels.append(label)

    tokenized_inputs['labels'] = labels
    return tokenized_inputs

# Step 4: Apply the tokenization and alignment to your dataset
dataset = dataset.map(tokenize_and_align_labels, batched=True)

# Step 5: Verify if the tokenization is correct
print(dataset['crpc'][0])  # Check a sample after processing

# Step 6: Set up the training arguments
training_args = TrainingArguments(
    output_dir='./results',
    evaluation_strategy="epoch",  # Evaluate at the end of each epoch
    learning_rate=2e-5,
    per_device_train_batch_size=16,  # You can adjust batch size based on your GPU/CPU memory
    per_device_eval_batch_size=16,   # You can adjust batch size based on your GPU/CPU memory
    num_train_epochs=3,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=10,
)

# Step 7: Initialize the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset['crpc'],  # Adjust to match your dataset
    eval_dataset=dataset['ipc'],  # Adjust to match your dataset
    tokenizer=tokenizer,
)

# Step 8: Start training
trainer.train()

# Step 7: Evaluate the model
results = trainer.evaluate()

# Step 8: Print evaluation results (accuracy)
print("Evaluation results: ", results)

!pip install evaluate

logging_dir='./logs',
logging_steps=10,  # Log after every 10 steps

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
# %tensorboard --logdir ./logs

# Save the model and tokenizer locally
model.save_pretrained('./legal_bert_finetuned')
tokenizer.save_pretrained('./legal_bert_finetuned')

from transformers import pipeline

# Load the trained model and tokenizer
nlp = pipeline("ner", model=model, tokenizer=tokenizer)

# Example legal text you want to classify
example_text = "The accused has been charged under Section 302 of the Indian Penal Code."

# Make predictions
predictions = nlp(example_text)

# Print predictions
print(predictions)

def visualize_predictions(text, predictions):
    for prediction in predictions:
        word = prediction['word']
        entity = prediction['entity']
        print(f"{word}: {entity}")

# Visualize the predictions for the example text
visualize_predictions(example_text, predictions)

# Example label mapping for NER output
label_map = {
    "LABEL_0": "O",  # No entity
    "LABEL_2": "Accused",  # Represents the accused
    "LABEL_4": "Charge",  # Represents a charge
    "LABEL_5": "Law",  # Represents legal terms or laws
    "LABEL_8": "Legal Code",  # Represents legal sections or codes
}

# Sample prediction output
predictions = [
    {'word': 'the', 'label': 'LABEL_5'},
    {'word': 'accused', 'label': 'LABEL_2'},
    {'word': 'has', 'label': 'LABEL_0'},
    {'word': 'been', 'label': 'LABEL_2'},
    {'word': 'charged', 'label': 'LABEL_5'},
    {'word': 'under', 'label': 'LABEL_4'},
    {'word': 'section', 'label': 'LABEL_8'},
    {'word': '302', 'label': 'LABEL_5'},
    {'word': 'of', 'label': 'LABEL_8'},
    {'word': 'the', 'label': 'LABEL_5'},
    {'word': 'indian', 'label': 'LABEL_8'},
    {'word': 'penal', 'label': 'LABEL_8'},
    {'word': 'code', 'label': 'LABEL_8'},
    {'word': '.', 'label': 'LABEL_2'}
]

# Map predictions to labels
def visualize_predictions(predictions, label_map):
    for prediction in predictions:
        word = prediction['word']
        label = prediction['label']
        entity = label_map.get(label, "Unknown")  # Get the entity name from the label
        print(f"Word: {word}, Entity: {entity}")

# Visualize the predictions with labels
visualize_predictions(predictions, label_map)

from transformers import pipeline

# Load the trained model and tokenizer
nlp = pipeline("ner", model=model, tokenizer=tokenizer)

# Example legal text you want to classify
example_text = "The accused has been charged under Section 302 of the Indian Penal Code."

# Make predictions
predictions = nlp(example_text)

# Print predictions
for prediction in predictions:
    print(f"Word: {prediction['word']}, Entity: {prediction['entity']}, Score: {prediction['score']}")

label2id = {
    "O": 0,
    "Accused": 1,
    "Charge": 2,
    "Law": 3,
    "Legal Code": 4,
    "Section": 5
}

id2label = {v: k for k, v in label2id.items()}

# Define the mapping of numeric labels to entity names
id2label = {
    0: "O",  # No entity
    1: "Accused",
    2: "Charge",
    3: "Law",
    4: "Legal Code",
    5: "Section"
}

# Example of predicted labels with numeric indices
predictions = [
    {'word': 'the', 'label': 'LABEL_3'},
    {'word': 'accused', 'label': 'LABEL_1'},
    {'word': 'has', 'label': 'LABEL_0'},
    {'word': 'been', 'label': 'LABEL_1'},
    {'word': 'charged', 'label': 'LABEL_3'},
    {'word': 'under', 'label': 'LABEL_4'},
    {'word': 'section', 'label': 'LABEL_5'},
    {'word': '302', 'label': 'LABEL_3'},
    {'word': 'of', 'label': 'LABEL_5'},
    {'word': 'the', 'label': 'LABEL_3'},
    {'word': 'indian', 'label': 'LABEL_5'},
    {'word': 'penal', 'label': 'LABEL_5'},
    {'word': 'code', 'label': 'LABEL_5'}
]

# Map predictions to labels using id2label
def visualize_predictions(predictions, id2label):
    for prediction in predictions:
        word = prediction['word']
        label = prediction['label']
        entity = id2label.get(int(label.split("_")[1]), "Unknown")  # Convert 'LABEL_X' to integer
        print(f"Word: {word}, Entity: {entity}")

# Visualize predictions
visualize_predictions(predictions, id2label)

# Example of multiple sentences (a batch)
texts = [
    "The accused has been charged under Section 302.",
    "The court has ruled in favor of the plaintiff."
]

# Tokenize the batch
inputs = tokenizer(texts, padding=True, truncation=True, return_tensors="pt")

print(inputs)
